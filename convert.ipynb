{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note Placement Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import platform\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from time import localtime, strftime\n",
    "\n",
    "is_macos = platform == 'darwin'\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeatmapConverter:\n",
    "    def __init__(self,\n",
    "                audio_path:Path, \n",
    "                osu_path:Path, \n",
    "                output_path:Path,\n",
    "                n_fft_list:list=[1024, 2048, 4096],\n",
    "                hop_ms:int=10,\n",
    "                context_window_size = 7,\n",
    "                beat_division = 48):\n",
    "        \n",
    "        self.audio_path = audio_path\n",
    "        self.osu_path = osu_path\n",
    "        self.output_path = output_path\n",
    "        self.hop_ms = hop_ms\n",
    "        self.hop_length = int(44100 * (hop_ms / 1000))\n",
    "        self.melspec_converters = [ torchaudio.transforms.MelSpectrogram(sample_rate=44100,\n",
    "                                                                        n_fft=n_fft, \n",
    "                                                                        hop_length=self.hop_length, \n",
    "                                                                        f_max=11000, \n",
    "                                                                        n_mels=80,\n",
    "                                                                        power=2) \n",
    "                                                                        for n_fft in n_fft_list ]\n",
    "        self.context_window_size = context_window_size\n",
    "        self.beat_division_length = round(1 / beat_division, 5)\n",
    "\n",
    "    def round_base(self, input, base):\n",
    "        if isinstance(input, float) or isinstance(input, int):\n",
    "            return base * round(input / base)\n",
    "        elif isinstance(input, torch.Tensor):\n",
    "            return (input / base).round() * base\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def get_beat_phase(self, note_time, offset:float, beat_length:float):\n",
    "        beat_phase = ((note_time - offset) % beat_length) / beat_length\n",
    "        return self.round_base(beat_phase, self.beat_division_length)\n",
    "\n",
    "    def parse_beatmap(self, fn):\n",
    "        # Extract beatmap information from an .osu file.\n",
    "        # RETURNS: FloatTensor: num_notes X 4(time, key_number, note_type, beat_phase).\n",
    "        #          -1 if error.\n",
    "\n",
    "        with open(fn, mode='r', encoding='utf-8') as f:\n",
    "            raw_content = f.read().splitlines()\n",
    "\n",
    "        # Get difficulty (round to .2)\n",
    "        difficulty = fn.name.split('-')[1]\n",
    "        difficulty = self.round_base(float(difficulty[:1] + '.' + difficulty[1:]), 0.2)\n",
    "\n",
    "        timing_points = []\n",
    "        # Read everything until next section\n",
    "        i = raw_content.index('[TimingPoints]') + 1\n",
    "        while raw_content[i] != '' and raw_content[i][0] != '[':\n",
    "            timing_points.append(raw_content[i])\n",
    "            i += 1\n",
    "\n",
    "        # Check if multiple BPMs exist\n",
    "        beat_lengths = {float(tp.split(',')[1]) for tp in timing_points if float(tp.split(',')[1]) > 0}\n",
    "        if len(beat_lengths) > 1:\n",
    "            return -1, -1, -1, -1, -1\n",
    "\n",
    "        offset = float(timing_points[0].split(',')[0])\n",
    "        beat_length = beat_lengths.pop()\n",
    "\n",
    "        beatmap_start_index = raw_content.index('[HitObjects]')\n",
    "        beatmap = raw_content[beatmap_start_index + 1:]\n",
    "\n",
    "        obj_list = []\n",
    "        xpos_set = set()\n",
    "\n",
    "        for obj in beatmap:\n",
    "            obj_split = obj.split(',')\n",
    "            time = int(obj_split[2])\n",
    "            xpos = int(obj_split[0])\n",
    "            xpos_set.add(xpos)\n",
    "\n",
    "            if obj_split[3] != '1': # If note is long note...\n",
    "                end_time = int(obj_split[5].split(':', 1)[0])\n",
    "                obj_list.append([time, xpos, 2, self.get_beat_phase(time, offset, beat_length)])\n",
    "                obj_list.append([end_time, xpos, 3, self.get_beat_phase(time, offset, beat_length)])\n",
    "            else:\n",
    "                obj_list.append([time, xpos, 1, self.get_beat_phase(time, offset, beat_length)])\n",
    "\n",
    "        xpos_list = sorted(xpos_set)\n",
    "        xpos2num = {xpos: num for num, xpos in enumerate(xpos_list)}\n",
    "\n",
    "        obj_list = [[obj[0], xpos2num[obj[1]], obj[2], obj[3]] for obj in obj_list]\n",
    "        obj_tensor = torch.tensor(obj_list, dtype=torch.float32)\n",
    "\n",
    "        # Sort by note time in ascending order\n",
    "        obj_tensor = obj_tensor[obj_tensor[:, 0].argsort()]\n",
    "\n",
    "        return len(xpos_list), obj_tensor, offset, beat_length, difficulty\n",
    "\n",
    "    def convert_audio(self, y, offset, beat_length, eps=1e-9):\n",
    "        # Converts audio into 3-channel mel-spectrogram with context windows.\n",
    "        # INPUT: waveform of sr=44100\n",
    "        # OUTPUT: Tensor([num_timesteps, len_window * 2 + 1, 80, 3])\n",
    "\n",
    "        # Multiple-timescale STFT\n",
    "        specs = []\n",
    "        for converter in self.melspec_converters:\n",
    "            melspec = converter(y + eps)\n",
    "            specs.append(torch.log(melspec.T))\n",
    "        specs = torch.stack(specs, dim=-1) # len X 80 X 3\n",
    "        min_value = torch.min(specs)\n",
    "\n",
    "        # Shift specs and stack\n",
    "        shifted_specs = []\n",
    "        for i in range(self.context_window_size, -self.context_window_size-1, -1):\n",
    "            shifted = torch.roll(specs, i, 0)\n",
    "            if i > 0:\n",
    "                shifted[:i, :, :] = min_value\n",
    "            elif i < 0:\n",
    "                shifted[i:, :, :] = min_value\n",
    "            shifted_specs.append(shifted)\n",
    "        shifted_specs = torch.stack(shifted_specs, dim=1) # len X 15 X 80 X 3\n",
    "\n",
    "        # Create beat phase tensor\n",
    "        beat_phase = self.get_beat_phase(torch.arange(len(shifted_specs)) * self.hop_ms, offset, beat_length)\n",
    "\n",
    "        return shifted_specs, beat_phase\n",
    "\n",
    "    def find_file_by_stem(self, fn_list, stem):\n",
    "        for fn in fn_list:\n",
    "            if fn.stem == stem:\n",
    "                return fn\n",
    "        return -1\n",
    "\n",
    "    def convert(self):\n",
    "        audio_fns = sorted([p for p in self.audio_path.glob('**/*') if p.suffix in {'.mp3', '.wav', '.ogg'}])\n",
    "        osu_fns = sorted(list(self.osu_path.glob('*.osu')))\n",
    "\n",
    "        converted_audio_path = self.output_path / 'converted_audio/'\n",
    "        converted_audio_path.mkdir(exist_ok=True)\n",
    "        num_keys_paths = []\n",
    "\n",
    "        excluded_audio_path = self.audio_path / 'excluded_audio/'\n",
    "        excluded_osu_path = self.osu_path / 'excluded_osu'\n",
    "        excluded_audio_path.mkdir(exist_ok=True)\n",
    "        excluded_osu_path.mkdir(exist_ok=True)\n",
    "        log = open(self.output_path / (strftime('conversion-log-%Y-%m-%d-%H-%M-%S', localtime()) + '.txt'), 'w')\n",
    "\n",
    "        for i, osu_fn in enumerate(osu_fns):\n",
    "            print(f'Converting: {osu_fn.name} ({i} out of {len(osu_fns)})', end='\\r', flush=True)\n",
    "            # Parse beatmap notes\n",
    "            num_keys, beatmap, offset, beat_length, difficulty = self.parse_beatmap(osu_fn)\n",
    "            if (num_keys == -1): # Error: beatmap has multiple BPMs\n",
    "                log.write(f'Multiple BPMs in file {osu_fn.name}: skipping conversion.\\n')\n",
    "                osu_fn.rename(excluded_osu_path / osu_fn.name)\n",
    "                osu_fns.remove(osu_fn)\n",
    "                audio_fn = self.find_file_by_stem(audio_fns, audio_stem)\n",
    "                if audio_fn != -1:\n",
    "                    audio_fn.rename(excluded_audio_path / audio_fn.name)\n",
    "                    audio_fns.remove(audio_fn)\n",
    "                continue\n",
    "\n",
    "            # Categorize created data samples into num_keys\n",
    "            num_keys_path = self.output_path / f'{num_keys}keys/'\n",
    "            if num_keys_path not in num_keys_paths:\n",
    "                num_keys_path.mkdir(exist_ok=True)\n",
    "                num_keys_paths.append(num_keys_path)\n",
    "\n",
    "            # Check if corresponding audio has already been converted\n",
    "            audio_stem = osu_fn.stem.split('-')[0]\n",
    "            converted_audio_fn = self.find_file_by_stem(list(converted_audio_path.glob('*.pt')), audio_stem)\n",
    "\n",
    "            if converted_audio_fn == -1:\n",
    "                # Load audio with OS-specific backend\n",
    "                audio_fn = self.find_file_by_stem(audio_fns, audio_stem)\n",
    "                if audio_fn == -1:\n",
    "                    log.write(f'Audio file not found: {audio_stem}\\n')\n",
    "                    osu_fn.rename(excluded_osu_path / osu_fn.name)\n",
    "                    osu_fns.remove(osu_fn)\n",
    "                    continue\n",
    "\n",
    "                if is_macos:\n",
    "                    y, sr = torchaudio.load(audio_fn, backend='ffmpeg')\n",
    "                else:\n",
    "                    y, sr = torchaudio.load(audio_fn)\n",
    "\n",
    "                y = y.mean(dim=0)\n",
    "                if sr != 44100:\n",
    "                    log.write(f'Sampling rate of file {audio_fn.name} is {sr}: Resampling to 44100.\\n')\n",
    "                    y = torchaudio.functional.resample(y, sr, 44100)\n",
    "\n",
    "                converted_audio, beat_phase = self.convert_audio(y, offset, beat_length)\n",
    "                torch.save({'audio': converted_audio, 'beat_phase': beat_phase}, (converted_audio_path / audio_fn.name).with_suffix('.pt'))\n",
    "\n",
    "            else:\n",
    "                converted_audio, beat_phase = torch.load(converted_audio_fn).values()\n",
    "        \n",
    "            torch.save({'audio': converted_audio, 'beat_phase': beat_phase, 'beatmap': beatmap, 'difficulty': difficulty}, (num_keys_path / osu_fn.name).with_suffix('.pt'))\n",
    "            \n",
    "        print('\\nConversion finished.')\n",
    "        log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion finished.08-7.osu (191 out of 192)\n"
     ]
    }
   ],
   "source": [
    "audio_path = Path('osu_dataset/original/')\n",
    "osu_path = Path('osu_dataset/original/')\n",
    "output_path = Path('osu_dataset/')\n",
    "\n",
    "converter = BeatmapConverter(audio_path, osu_path, output_path)\n",
    "converter.convert()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
