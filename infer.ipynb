{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, unpad_sequence, pack_sequence, pad_packed_sequence\n",
    "from torchmetrics.classification import BinaryF1Score\n",
    "from torchmetrics.functional.text.perplexity import perplexity\n",
    "from tqdm.auto import tqdm\n",
    "from utils import index_to_combination\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "RANDOM_SEED = 1\n",
    "DEV = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OsuModel(nn.Module):\n",
    "    def __init__(self, device, bp_emb_dim=16, bn_emb_dim=8, diff_emb_dim=8,\n",
    "                 np_hidden_size=256, np_num_layers=2, ns_pre_proj_size=32,\n",
    "                 ns_hidden_size=256, ns_num_layers=2, num_tokens=256):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.gelu = nn.GELU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.conv = nn.Conv2d(in_channels=3,\n",
    "                              out_channels=1,\n",
    "                              kernel_size=(15, 5),\n",
    "                              padding='same')\n",
    "        self.beat_phase_emb = nn.Embedding(49, bp_emb_dim)\n",
    "        self.beat_num_emb = nn.Embedding(4, bn_emb_dim)\n",
    "        self.difficulty_emb = nn.Embedding(21, diff_emb_dim)\n",
    "\n",
    "        self.np_gru = nn.GRU(input_size=80 + bp_emb_dim + bn_emb_dim + diff_emb_dim,\n",
    "                             hidden_size=np_hidden_size,\n",
    "                             num_layers=np_num_layers,\n",
    "                             batch_first=True,\n",
    "                             bidirectional=True)\n",
    "        self.np_proj_1 = nn.Linear(np_hidden_size*2, 128)\n",
    "        self.np_proj_2 = nn.Linear(128, 1)\n",
    "\n",
    "        self.ns_pre_proj = nn.Linear(128, ns_pre_proj_size)\n",
    "        self.ns_gru = nn.GRU(input_size=80 + ns_pre_proj_size + bp_emb_dim + bn_emb_dim + diff_emb_dim,\n",
    "                             hidden_size=ns_hidden_size,\n",
    "                             num_layers=ns_num_layers,\n",
    "                             batch_first=True,\n",
    "                             bidirectional=False)\n",
    "        self.ns_proj_1 = nn.Linear(ns_hidden_size, ns_hidden_size)\n",
    "        self.ns_proj_2 = nn.Linear(ns_hidden_size, num_tokens)\n",
    "\n",
    "    def forward(self, specs, beat_phases, beat_nums, difficulties, lengths):\n",
    "        conv_outs = [self.gelu(self.conv(spec)).squeeze() for spec in specs]\n",
    "        bp_emb = unpad_sequence(self.beat_phase_emb(beat_phases).to('cpu'),\n",
    "                                lengths.to('cpu'), batch_first=True)\n",
    "        bn_emb = unpad_sequence(self.beat_num_emb(beat_nums).to('cpu'),\n",
    "                                lengths.to('cpu'), batch_first=True)\n",
    "        diff = difficulties.unsqueeze(1).expand(-1, lengths.max().item())\n",
    "        diff_emb = unpad_sequence(self.difficulty_emb(diff).to('cpu'),\n",
    "                                  lengths.to('cpu'), batch_first=True)\n",
    "\n",
    "        # ========== Note Placement ========== #\n",
    "\n",
    "        np_in = []\n",
    "        for i in range(len(lengths)):\n",
    "            np_in.append(torch.cat([conv_outs[i],\n",
    "                                    bp_emb[i].to(self.device),\n",
    "                                    bn_emb[i].to(self.device),\n",
    "                                    diff_emb[i].to(self.device)],\n",
    "                                   dim=-1))\n",
    "        np_in_packed = pack_sequence(np_in, enforce_sorted=False)\n",
    "        np_out, last_hidden = self.np_gru(np_in_packed)\n",
    "        np_out_padded, _ = pad_packed_sequence(np_out, batch_first=True)\n",
    "\n",
    "        np_proj_1_out = self.gelu(self.np_proj_1(np_out_padded))\n",
    "        np_pred = self.sigmoid(self.np_proj_2(np_proj_1_out)).squeeze()\n",
    "\n",
    "        # ========== Note Selection ========== #\n",
    "\n",
    "        ns_pre_proj_padded = self.ns_pre_proj(np_proj_1_out)\n",
    "        ns_pre_proj = unpad_sequence(ns_pre_proj_padded, lengths, batch_first=True)\n",
    "        ns_in = []\n",
    "        for i in range(len(lengths)):\n",
    "            ns_in.append(torch.cat([conv_outs[i],\n",
    "                                    ns_pre_proj[i],\n",
    "                                    bp_emb[i].to(self.device),\n",
    "                                    bn_emb[i].to(self.device),\n",
    "                                    diff_emb[i].to(self.device)],\n",
    "                                   dim=-1))\n",
    "        ns_in_packed = pack_sequence(ns_in, enforce_sorted=False)\n",
    "        ns_out, last_hidden = self.ns_gru(ns_in_packed)\n",
    "        ns_out_padded, _ = pad_packed_sequence(ns_out, batch_first=True)\n",
    "\n",
    "        ns_proj_1_out = self.gelu(self.ns_proj_1(ns_out_padded))\n",
    "        ns_logit = self.ns_proj_2(ns_proj_1_out)\n",
    "\n",
    "        return np_pred, ns_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_size):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_size, 3, padding=1),\n",
    "            nn.LayerNorm(80),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_size, in_channels, 1),\n",
    "            nn.LayerNorm(80),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x) + x\n",
    "\n",
    "class ConvStack(nn.Module):\n",
    "    def __init__(self, num_blocks, in_channels, hidden_size, out_channels):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.first_conv = nn.Conv2d(in_channels, hidden_size, 1)\n",
    "        self.first_norm = nn.LayerNorm(80)\n",
    "\n",
    "        self.stack = nn.ModuleList([\n",
    "            ConvBlock(hidden_size, hidden_size) for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.last_conv = nn.Conv2d(hidden_size, out_channels, 1)\n",
    "        self.last_norm = nn.LayerNorm(80)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.first_norm(self.first_conv(x)))\n",
    "        for block in self.stack:\n",
    "            x = block(x)\n",
    "        return self.relu(self.last_norm(self.last_conv(x)))\n",
    "\n",
    "class StackModel(nn.Module):\n",
    "    def __init__(self, device, num_stacks=7, bp_emb_dim=16, bn_emb_dim=8, diff_emb_dim=8,\n",
    "                 np_hidden_size=256, np_num_layers=2, ns_pre_proj_size=32,\n",
    "                 ns_hidden_size=256, ns_num_layers=2, num_tokens=256):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.gelu = nn.GELU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.stack = ConvStack(num_stacks, 3, 16, 1)\n",
    "        self.beat_phase_emb = nn.Embedding(49, bp_emb_dim)\n",
    "        self.beat_num_emb = nn.Embedding(4, bn_emb_dim)\n",
    "        self.difficulty_emb = nn.Embedding(21, diff_emb_dim)\n",
    "\n",
    "        self.np_gru = nn.GRU(input_size=80 + bp_emb_dim + bn_emb_dim + diff_emb_dim,\n",
    "                             hidden_size=np_hidden_size,\n",
    "                             num_layers=np_num_layers,\n",
    "                             batch_first=True,\n",
    "                             bidirectional=True)\n",
    "        self.np_proj_1 = nn.Linear(np_hidden_size*2, 128)\n",
    "        self.np_proj_2 = nn.Linear(128, 1)\n",
    "\n",
    "        self.ns_pre_proj = nn.Linear(128, ns_pre_proj_size)\n",
    "        self.ns_gru = nn.GRU(input_size=80 + ns_pre_proj_size + bp_emb_dim + bn_emb_dim + diff_emb_dim,\n",
    "                             hidden_size=ns_hidden_size,\n",
    "                             num_layers=ns_num_layers,\n",
    "                             batch_first=True,\n",
    "                             bidirectional=False)\n",
    "        self.ns_proj_1 = nn.Linear(ns_hidden_size, ns_hidden_size)\n",
    "        self.ns_proj_2 = nn.Linear(ns_hidden_size, num_tokens)\n",
    "\n",
    "    def forward(self, specs, beat_phases, beat_nums, difficulties, lengths):\n",
    "        conv_outs = [self.stack(spec).squeeze() for spec in specs]\n",
    "        bp_emb = unpad_sequence(self.beat_phase_emb(beat_phases).to('cpu'),\n",
    "                                lengths.to('cpu'), batch_first=True)\n",
    "        bn_emb = unpad_sequence(self.beat_num_emb(beat_nums).to('cpu'),\n",
    "                                lengths.to('cpu'), batch_first=True)\n",
    "        diff = difficulties.unsqueeze(1).expand(-1, lengths.max().item())\n",
    "        diff_emb = unpad_sequence(self.difficulty_emb(diff).to('cpu'),\n",
    "                                  lengths.to('cpu'), batch_first=True)\n",
    "\n",
    "        # ========== Note Placement ========== #\n",
    "\n",
    "        np_in = []\n",
    "        for i in range(len(lengths)):\n",
    "            np_in.append(torch.cat([conv_outs[i],\n",
    "                                    bp_emb[i].to(self.device),\n",
    "                                    bn_emb[i].to(self.device),\n",
    "                                    diff_emb[i].to(self.device)],\n",
    "                                   dim=-1))\n",
    "        np_in_packed = pack_sequence(np_in, enforce_sorted=False)\n",
    "        np_out, last_hidden = self.np_gru(np_in_packed)\n",
    "        np_out_padded, _ = pad_packed_sequence(np_out, batch_first=True)\n",
    "\n",
    "        np_proj_1_out = self.gelu(self.np_proj_1(np_out_padded))\n",
    "        np_pred = self.sigmoid(self.np_proj_2(np_proj_1_out)).squeeze()\n",
    "\n",
    "        # ========== Note Selection ========== #\n",
    "\n",
    "        ns_pre_proj_padded = self.ns_pre_proj(np_proj_1_out)\n",
    "        ns_pre_proj = unpad_sequence(ns_pre_proj_padded, lengths, batch_first=True)\n",
    "        ns_in = []\n",
    "        for i in range(len(lengths)):\n",
    "            ns_in.append(torch.cat([conv_outs[i],\n",
    "                                    ns_pre_proj[i],\n",
    "                                    bp_emb[i].to(self.device),\n",
    "                                    bn_emb[i].to(self.device),\n",
    "                                    diff_emb[i].to(self.device)],\n",
    "                                   dim=-1))\n",
    "        ns_in_packed = pack_sequence(ns_in, enforce_sorted=False)\n",
    "        ns_out, last_hidden = self.ns_gru(ns_in_packed)\n",
    "        ns_out_padded, _ = pad_packed_sequence(ns_out, batch_first=True)\n",
    "\n",
    "        ns_proj_1_out = self.gelu(self.ns_proj_1(ns_out_padded))\n",
    "        ns_logit = self.ns_proj_2(ns_proj_1_out)\n",
    "\n",
    "        return np_pred, ns_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, specs, beat_phases, beat_nums, difficulty, device):\n",
    "    assert specs.shape[1] == beat_phases.shape[0] == beat_nums.shape[0]\n",
    "    specs = specs.unsqueeze(0).to(device)\n",
    "    beat_phases = beat_phases.unsqueeze(0).to(device)\n",
    "    beat_nums = beat_nums.unsqueeze(0).to(device)\n",
    "    diff = torch.tensor([difficulty]).to(device)\n",
    "    lengths = torch.tensor([specs.shape[2]])\n",
    "\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        np_pred, ns_logit = model(specs, beat_phases, beat_nums, diff, lengths)\n",
    "        np_pred = np_pred.squeeze().round().bool()\n",
    "        ns_pred = ns_logit.squeeze().softmax(dim=-1)\n",
    "        ns_pred_idx = ns_pred.argmax(dim=-1)\n",
    "        \n",
    "        return np_pred, ns_pred_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StackModel(DEV)\n",
    "model.to(DEV)\n",
    "state = torch.load(Path('checkpoints/convstack/07-17-22-26-31-epoch14.pt'), map_location=DEV)\n",
    "model.load_state_dict(state['model_state_dict'])\n",
    "actions, onsets, _, difficulty = torch.load(Path('osu_dataset/beatmap/4keys/1008060-202-4.pt')).values()\n",
    "specs, beat_phase, beat_num = torch.load(Path('osu_dataset/audio/1008060.pt')).values()\n",
    "np_pred, ns_pred_idx = infer(model, specs, beat_phase, beat_num, 11, DEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1014, device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np_pred > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(646, device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ns_pred_idx > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 0, 0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_combination(5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "beatmap = []\n",
    "for i, token in enumerate(ns_pred_idx):\n",
    "    if token.item() > 0:\n",
    "        key0, key1, key2, key3 = index_to_combination(token.item(), 4)\n",
    "        beatmap.append([i * 10, key0, key1, key2, key3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "beatmap_str_list = []\n",
    "for action in beatmap:\n",
    "    time = action[0]\n",
    "    keys = action[1:]\n",
    "    for idx, key in enumerate(keys):\n",
    "        if key > 0:\n",
    "            if key == 2:\n",
    "                print('long note start!!')\n",
    "            if key == 3:\n",
    "                print('long note end!!')\n",
    "            xpos = 64 + idx * 128\n",
    "            beatmap_str_list.append(f'{xpos},192,{time},1,0,0:0:0:0:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False,  ..., False, False, False], device='cuda:0')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "beatmap_str_list = []\n",
    "for idx, action in enumerate(np_pred):\n",
    "    if action.item():\n",
    "        xpos = 64 + random.randint(0, 3) * 128\n",
    "        beatmap_str_list.append(f'{xpos},192,{idx * 10},1,0,0:0:0:0:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('beatmap.txt', 'w')\n",
    "file.write('\\n'.join(beatmap_str_list))\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
