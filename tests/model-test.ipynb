{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note Placement Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import platform\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "is_macos = platform == 'darwin'\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeatmapDataset:\n",
    "    def __init__(self, audio_dir, osu_dir, data_dir):\n",
    "        audio_fns = sorted([p for p in Path(audio_dir).glob('**/*') if p.suffix in {'.mp3', '.wav', '.ogg'}])\n",
    "        osu_fns = sorted(list(Path(osu_dir).rglob('*.osu')))\n",
    "        self.data_fns = list(zip(audio_fns, osu_fns))\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "    def get_beat_phase(self, note_time, beat_length:float, offset:int):\n",
    "        return ((note_time - offset) % beat_length) / beat_length\n",
    "\n",
    "    def parse_beatmap(self, fn):\n",
    "        # Extract beatmap information from an .osu file.\n",
    "        # RETURNS: FloatTensor: num_notes X 4(time, key_number, note_type, beat_phase). -1 if error.\n",
    "\n",
    "        with open(fn, mode='r', encoding='utf-8') as f:\n",
    "            raw_content = f.read().splitlines()\n",
    "\n",
    "        timing_points = []\n",
    "        # Read everything until next section\n",
    "        i = raw_content.index('[TimingPoints]') + 1\n",
    "        while raw_content[i] != '' and raw_content[i][0] != '[':\n",
    "            timing_points.append(raw_content[i])\n",
    "            i += 1\n",
    "\n",
    "        # Check if multiple BPMs exist\n",
    "        beat_lengths = {float(tp.split(',')[1]) for tp in timing_points if float(tp.split(',')[1]) > 0}\n",
    "        if len(beat_lengths) > 1:\n",
    "            warnings.warn(f'Multiple BPMs in file {fn.name}: skipping conversion.')\n",
    "            return -1\n",
    "\n",
    "        offset = int(timing_points[0].split(',')[0])\n",
    "        beat_length = beat_lengths.pop()\n",
    "\n",
    "        beatmap_start_index = raw_content.index('[HitObjects]')\n",
    "        beatmap = raw_content[beatmap_start_index + 1:]\n",
    "\n",
    "        obj_list = []\n",
    "        xpos_set = set()\n",
    "\n",
    "        for obj in beatmap:\n",
    "            obj_split = obj.split(',')\n",
    "            time = int(obj_split[2])\n",
    "            xpos = int(obj_split[0])\n",
    "            xpos_set.add(xpos)\n",
    "\n",
    "            if obj_split[3] != '1': # If note is long note...\n",
    "                end_time = int(obj_split[5].split(':', 1)[0])\n",
    "                obj_list.append([time, xpos, 2, self.get_beat_phase(time, beat_length, offset)])\n",
    "                obj_list.append([end_time, xpos, 3, self.get_beat_phase(time, beat_length, offset)])\n",
    "            else:\n",
    "                obj_list.append([time, xpos, 1, self.get_beat_phase(time, beat_length, offset)])\n",
    "\n",
    "        xpos_list = sorted(xpos_set)\n",
    "        xpos2num = {xpos: num for num, xpos in enumerate(xpos_list)}\n",
    "\n",
    "        obj_list = [[obj[0], xpos2num[obj[1]], obj[2], obj[3]] for obj in obj_list]\n",
    "        obj_tensor = torch.tensor(obj_list, dtype=torch.float32)\n",
    "\n",
    "        # Sort by note time in ascending order\n",
    "        obj_tensor = obj_tensor[obj_tensor[:, 0].argsort()]\n",
    "\n",
    "        return obj_tensor\n",
    "    \n",
    "    # def shift_cat_audio(self, y, shift_amount):\n",
    "    #     tensor_list = []\n",
    "    #     for shift in range(shift_amount, 0, -1):\n",
    "    #         shifted = y.\n",
    "\n",
    "    def convert_spec(self, n_fft_list:list=[1024, 2048, 4096], hop_ms:int=10, n_mels:int=80):\n",
    "\n",
    "        # TODO: split melspec into 15 sample windows\n",
    "        # TODO: include beat phase\n",
    "\n",
    "        melspec_converters = [ torchaudio.transforms.MelSpectrogram(sample_rate=44100, n_fft=n_fft, hop_length=int(44100*(hop_ms/1000)), f_max=11000, n_mels=80) for n_fft in n_fft_list ]\n",
    "        db_converter = torchaudio.transforms.AmplitudeToDB()\n",
    "\n",
    "        for audio_fn, osu_fn in self.data_fns:\n",
    "            if is_macos:\n",
    "                y, sr = torchaudio.load(audio_fn, backend='ffmpeg')\n",
    "            else:\n",
    "                y, sr = torchaudio.load(audio_fn)\n",
    "            y = y.mean(dim=0)\n",
    "            \n",
    "            if sr != 44100:\n",
    "                print(f'Sampling rate of file {audio_fn.name} is {sr}: Resampling to 44100.')\n",
    "                y = torchaudio.functional.resample(y, sr, 44100)\n",
    "            \n",
    "            # Multiple-timescale STFT\n",
    "            specs = []\n",
    "            for converter in melspec_converters:\n",
    "                melspec = converter(y)\n",
    "                specs.append(db_converter(melspec))\n",
    "            specs = torch.stack(specs, dim=-1)\n",
    "\n",
    "            # Parse beatmap\n",
    "            print(osu_fn)\n",
    "            beatmap = self.parse_beatmap(osu_fn)\n",
    "\n",
    "            torch.save({'specs': specs, 'beatmap': beatmap}, (self.data_dir / osu_fn.name).with_suffix('.pt'))\n",
    "\n",
    "    # TODO: getitem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/converted/er.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m specs, beatmap \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mdata/converted/er.pt\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mvalues()\n\u001b[1;32m      2\u001b[0m specs\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/lib/python3.10/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/lib/python3.10/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    272\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/lib/python3.10/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/converted/er.pt'"
     ]
    }
   ],
   "source": [
    "specs, beatmap = torch.load('data/converted/er.pt').values()\n",
    "specs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([388, 4])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = BeatmapDataset('data/', 'data/', 'data/')\n",
    "dataset.parse_beatmap('data/er.osu').shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
