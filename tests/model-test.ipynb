{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note Placement Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import platform\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "is_macos = platform == 'darwin'\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeatmapConverter:\n",
    "    def __init__(self,\n",
    "                audio_path:Path, \n",
    "                osu_path:Path, \n",
    "                output_path:Path,\n",
    "                n_fft_list:list=[1024, 2048, 4096],\n",
    "                hop_ms:int=10,\n",
    "                context_window_size = 7):\n",
    "        \n",
    "        self.audio_path = audio_path\n",
    "        self.osu_path = osu_path\n",
    "        self.output_path = output_path\n",
    "        self.hop_ms = hop_ms\n",
    "        self.hop_length = int(44100 * (hop_ms / 1000))\n",
    "        self.melspec_converters = [ torchaudio.transforms.MelSpectrogram(sample_rate=44100,\n",
    "                                                                        n_fft=n_fft, \n",
    "                                                                        hop_length=self.hop_length, \n",
    "                                                                        f_max=11000, \n",
    "                                                                        n_mels=80,\n",
    "                                                                        power=2) \n",
    "                                                                        for n_fft in n_fft_list ]\n",
    "        self.context_window_size = context_window_size\n",
    "\n",
    "    def round_base(self, input, base):\n",
    "        if isinstance(input, float) or isinstance(input, int):\n",
    "            return base * round(input / base)\n",
    "        elif isinstance(input, torch.Tensor):\n",
    "            return (input / base).round() * base\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def get_beat_phase(self, note_time, offset:float, beat_length:float):\n",
    "        beat_phase = ((note_time - offset) % beat_length) / beat_length\n",
    "        return self.round_base(beat_phase, 0.02083) # 1/48\n",
    "\n",
    "    def parse_beatmap(self, fn):\n",
    "        # Extract beatmap information from an .osu file.\n",
    "        # RETURNS: FloatTensor: num_notes X 4(time, key_number, note_type, beat_phase).\n",
    "        #          -1 if error.\n",
    "\n",
    "        with open(fn, mode='r', encoding='utf-8') as f:\n",
    "            raw_content = f.read().splitlines()\n",
    "\n",
    "        # Get difficulty (round to .2)\n",
    "        difficulty = fn.name.split('-')[1]\n",
    "        difficulty = self.round_base(float(difficulty[:1] + '.' + difficulty[1:]), 0.2)\n",
    "\n",
    "        timing_points = []\n",
    "        # Read everything until next section\n",
    "        i = raw_content.index('[TimingPoints]') + 1\n",
    "        while raw_content[i] != '' and raw_content[i][0] != '[':\n",
    "            timing_points.append(raw_content[i])\n",
    "            i += 1\n",
    "\n",
    "        # Check if multiple BPMs exist\n",
    "        beat_lengths = {float(tp.split(',')[1]) for tp in timing_points if float(tp.split(',')[1]) > 0}\n",
    "        if len(beat_lengths) > 1:\n",
    "            print(f'Multiple BPMs in file {fn.name}: skipping conversion.')\n",
    "            return -1, -1, -1, -1, -1\n",
    "\n",
    "        offset = float(timing_points[0].split(',')[0])\n",
    "        beat_length = beat_lengths.pop()\n",
    "\n",
    "        beatmap_start_index = raw_content.index('[HitObjects]')\n",
    "        beatmap = raw_content[beatmap_start_index + 1:]\n",
    "\n",
    "        obj_list = []\n",
    "        xpos_set = set()\n",
    "\n",
    "        for obj in beatmap:\n",
    "            obj_split = obj.split(',')\n",
    "            time = int(obj_split[2])\n",
    "            xpos = int(obj_split[0])\n",
    "            xpos_set.add(xpos)\n",
    "\n",
    "            if obj_split[3] != '1': # If note is long note...\n",
    "                end_time = int(obj_split[5].split(':', 1)[0])\n",
    "                obj_list.append([time, xpos, 2, self.get_beat_phase(time, offset, beat_length)])\n",
    "                obj_list.append([end_time, xpos, 3, self.get_beat_phase(time, offset, beat_length)])\n",
    "            else:\n",
    "                obj_list.append([time, xpos, 1, self.get_beat_phase(time, offset, beat_length)])\n",
    "\n",
    "        xpos_list = sorted(xpos_set)\n",
    "        xpos2num = {xpos: num for num, xpos in enumerate(xpos_list)}\n",
    "\n",
    "        obj_list = [[obj[0], xpos2num[obj[1]], obj[2], obj[3]] for obj in obj_list]\n",
    "        obj_tensor = torch.tensor(obj_list, dtype=torch.float32)\n",
    "\n",
    "        # Sort by note time in ascending order\n",
    "        obj_tensor = obj_tensor[obj_tensor[:, 0].argsort()]\n",
    "\n",
    "        return len(xpos_list), obj_tensor, offset, beat_length, difficulty\n",
    "\n",
    "    def convert_audio(self, y, offset, beat_length):\n",
    "        # Converts audio into 3-channel mel-spectrogram with context windows.\n",
    "        # INPUT: waveform of sr=44100\n",
    "        # OUTPUT: Tensor([num_timesteps, len_window * 2 + 1, 80, 3])\n",
    "\n",
    "        # Multiple-timescale STFT\n",
    "        specs = []\n",
    "        for converter in self.melspec_converters:\n",
    "            melspec = converter(y)\n",
    "            specs.append(torch.log(melspec.T))\n",
    "        specs = torch.stack(specs, dim=-1) # len X 80 X 3\n",
    "        min_value = torch.min(specs)\n",
    "\n",
    "        # Shift specs and stack\n",
    "        shifted_specs = []\n",
    "        for i in range(self.context_window_size, -self.context_window_size-1, -1):\n",
    "            shifted = torch.roll(specs, i, 0)\n",
    "            if i > 0:\n",
    "                shifted[:i, :, :] = min_value\n",
    "            elif i < 0:\n",
    "                shifted[i:, :, :] = min_value\n",
    "            shifted_specs.append(shifted)\n",
    "        shifted_specs = torch.stack(shifted_specs, dim=1) # len X 15 X 80 X 3\n",
    "\n",
    "        # Create beat phase tensor\n",
    "        beat_phase = self.get_beat_phase(torch.arange(len(shifted_specs)) * self.hop_ms, offset, beat_length)\n",
    "\n",
    "        return shifted_specs, beat_phase\n",
    "\n",
    "    def find_file_by_stem(self, fn_list, stem):\n",
    "        for fn in fn_list:\n",
    "            if fn.stem == stem:\n",
    "                return fn\n",
    "        return -1\n",
    "\n",
    "    def convert(self):\n",
    "        audio_fns = sorted([p for p in self.audio_path.glob('**/*') if p.suffix in {'.mp3', '.wav', '.ogg'}])\n",
    "        osu_fns = sorted(list(self.osu_path.rglob('*.osu')))\n",
    "\n",
    "        converted_audio_path = self.output_path / 'converted_audio/'\n",
    "        converted_audio_path.mkdir(exist_ok=True)\n",
    "        num_keys_paths = []\n",
    "\n",
    "        excluded_audio_path = self.audio_path / 'excluded_audio/'\n",
    "        excluded_osu_path = self.osu_path / 'excluded_osu'\n",
    "        excluded_audio_path.mkdir(exist_ok=True)\n",
    "        excluded_osu_path.mkdir(exist_ok=True)\n",
    "\n",
    "        for osu_fn in tqdm(osu_fns):\n",
    "            # Parse beatmap notes\n",
    "            num_keys, beatmap, offset, beat_length, difficulty = self.parse_beatmap(osu_fn)\n",
    "            if (num_keys == -1): # Error: beatmap has multiple BPMs\n",
    "                osu_fn.rename(excluded_osu_path / osu_fn.name)\n",
    "                audio_fn = self.find_file_by_stem(audio_fns, audio_stem)\n",
    "                if audio_fn != -1:\n",
    "                    audio_fn.rename(excluded_audio_path / audio_fn.name)\n",
    "                continue\n",
    "\n",
    "            # Categorize created data samples into num_keys\n",
    "            num_keys_path = self.output_path / f'{num_keys}keys/'\n",
    "            if num_keys_path not in num_keys_paths:\n",
    "                num_keys_path.mkdir(exist_ok=True)\n",
    "                num_keys_paths.append(num_keys_path)\n",
    "\n",
    "            # Check if corresponding audio has already been converted\n",
    "            audio_stem = osu_fn.stem.split('-')[0]\n",
    "            converted_audio_fn = self.find_file_by_stem(list(converted_audio_path.glob('*.pt')), audio_stem)\n",
    "\n",
    "            if converted_audio_fn == -1:\n",
    "                # Load audio with OS-specific backend\n",
    "                audio_fn = self.find_file_by_stem(audio_fns, audio_stem)\n",
    "                if audio_fn == -1:\n",
    "                    print(f'Audio file not found: {audio_stem}')\n",
    "                    osu_fn.rename(excluded_osu_path / osu_fn.name)\n",
    "                    continue\n",
    "\n",
    "                if is_macos:\n",
    "                    y, sr = torchaudio.load(audio_fn, backend='ffmpeg')\n",
    "                else:\n",
    "                    y, sr = torchaudio.load(audio_fn)\n",
    "\n",
    "                y = y.mean(dim=0)\n",
    "                if sr != 44100:\n",
    "                    print(f'Sampling rate of file {audio_fn.name} is {sr}: Resampling to 44100.')\n",
    "                    y = torchaudio.functional.resample(y, sr, 44100)\n",
    "\n",
    "                converted_audio, beat_phase = self.convert_audio(y, offset, beat_length)\n",
    "                torch.save({'audio': converted_audio, 'beat_phase': beat_phase}, (converted_audio_path / audio_fn.name).with_suffix('.pt'))\n",
    "\n",
    "            else:\n",
    "                converted_audio, beat_phase = torch.load(converted_audio_fn).values()\n",
    "        \n",
    "            torch.save({'audio': converted_audio, 'beat_phase': beat_phase, 'beatmap': beatmap, 'difficulty': difficulty}, (num_keys_path / osu_fn.name).with_suffix('.pt'))\n",
    "\n",
    "# TODO: define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17149811b92646ada3e35bf5c783929e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file not found: 106212\n",
      "Audio file not found: 106212\n",
      "Sampling rate of file 1276702.mp3 is 48000: Resampling to 44100.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m output_path \u001b[39m=\u001b[39m Path(\u001b[39m'\u001b[39m\u001b[39m../osu_dataset/\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m converter \u001b[39m=\u001b[39m BeatmapConverter(audio_path, osu_path, output_path)\n\u001b[0;32m----> 6\u001b[0m converter\u001b[39m.\u001b[39;49mconvert()\n",
      "Cell \u001b[0;32mIn[34], line 185\u001b[0m, in \u001b[0;36mBeatmapConverter.convert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     converted_audio, beat_phase \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(converted_audio_fn)\u001b[39m.\u001b[39mvalues()\n\u001b[0;32m--> 185\u001b[0m torch\u001b[39m.\u001b[39;49msave({\u001b[39m'\u001b[39;49m\u001b[39maudio\u001b[39;49m\u001b[39m'\u001b[39;49m: converted_audio, \u001b[39m'\u001b[39;49m\u001b[39mbeat_phase\u001b[39;49m\u001b[39m'\u001b[39;49m: beat_phase, \u001b[39m'\u001b[39;49m\u001b[39mbeatmap\u001b[39;49m\u001b[39m'\u001b[39;49m: beatmap, \u001b[39m'\u001b[39;49m\u001b[39mdifficulty\u001b[39;49m\u001b[39m'\u001b[39;49m: difficulty}, (num_keys_path \u001b[39m/\u001b[39;49m osu_fn\u001b[39m.\u001b[39;49mname)\u001b[39m.\u001b[39;49mwith_suffix(\u001b[39m'\u001b[39;49m\u001b[39m.pt\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/lib/python3.10/site-packages/torch/serialization.py:441\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    440\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    442\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/lib/python3.10/site-packages/torch/serialization.py:668\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[39m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[1;32m    667\u001b[0m num_bytes \u001b[39m=\u001b[39m storage\u001b[39m.\u001b[39mnbytes()\n\u001b[0;32m--> 668\u001b[0m zip_file\u001b[39m.\u001b[39;49mwrite_record(name, storage\u001b[39m.\u001b[39;49mdata_ptr(), num_bytes)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "audio_path = Path('../osu_dataset/original/')\n",
    "osu_path = Path('../osu_dataset/original/')\n",
    "output_path = Path('../osu_dataset/')\n",
    "\n",
    "converter = BeatmapConverter(audio_path, osu_path, output_path)\n",
    "converter.convert()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
