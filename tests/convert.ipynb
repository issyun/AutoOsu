{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beatmap + audio conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import platform\n",
    "from pathlib import Path\n",
    "from time import localtime, strftime\n",
    "from math import floor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "is_macos = platform == 'darwin'\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeatmapConverter:\n",
    "    def __init__(self,\n",
    "                osu_path:Path,\n",
    "                audio_path:Path,\n",
    "                beatmap_output_path:Path,\n",
    "                audio_output_path:Path,\n",
    "                n_fft_list:list=[1024, 2048, 4096],\n",
    "                hop_ms:int=10,\n",
    "                beat_division = 48):\n",
    "        \n",
    "        self.osu_path = osu_path\n",
    "        self.audio_path = audio_path\n",
    "        self.beatmap_output_path = beatmap_output_path\n",
    "        self.audio_output_path = audio_output_path\n",
    "        self.hop_ms = hop_ms\n",
    "        self.hop_length = int(44100 * (hop_ms / 1000))\n",
    "        self.melspec_converters = [ torchaudio.transforms.MelSpectrogram(sample_rate=44100,\n",
    "                                                                        n_fft=n_fft, \n",
    "                                                                        hop_length=self.hop_length, \n",
    "                                                                        f_max=11000, \n",
    "                                                                        n_mels=80,\n",
    "                                                                        power=2) \n",
    "                                                                        for n_fft in n_fft_list ]\n",
    "        self.beat_division_length = round(1 / beat_division, 5)\n",
    "\n",
    "    def round_base(self, input, base, index=False):\n",
    "        if isinstance(input, float) or isinstance(input, int):\n",
    "            return round(input / base) if index else base * round(input / base)\n",
    "        elif isinstance(input, torch.Tensor):\n",
    "            return (input / base).round() if index else (input / base).round() * base\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def get_beat_phase(self, time, offset:float, beat_length:float):\n",
    "        beat_phase = ((time - offset) % beat_length) / beat_length\n",
    "        return self.round_base(beat_phase, self.beat_division_length, index=True)\n",
    "    \n",
    "    def get_beat_num(self, time, beat_length, meter, anchor):\n",
    "        beat_num = ((time - anchor) / beat_length) % meter\n",
    "        if isinstance(beat_num, torch.Tensor):\n",
    "                beat_num = beat_num.floor()\n",
    "        elif isinstance(beat_num, int) or isinstance(beat_num, float):\n",
    "                beat_num = floor(beat_num)\n",
    "        return beat_num\n",
    "\n",
    "    def parse_beatmap(self, fn):\n",
    "        \"\"\"\n",
    "        Extract beatmap beat objects from an .osu file.\n",
    "        RETURNS: Beatmap: Tensor([num_notes X 3(time, key_number, note_type)]),\n",
    "                 num_keys, offset, beat_length, difficulty\n",
    "                 (-1 if error)\n",
    "        \"\"\"\n",
    "\n",
    "        with open(fn, mode='r', encoding='utf-8') as f:\n",
    "            raw_content = f.read().splitlines()\n",
    "\n",
    "        # Get difficulty (round to .2)\n",
    "        difficulty = fn.name.split('-')[1]\n",
    "        difficulty = self.round_base(float(difficulty[:1] + '.' + difficulty[1:]), 0.2)\n",
    "\n",
    "        # Get timing points\n",
    "        timing_points = []\n",
    "        # Read everything until next section\n",
    "        i = raw_content.index('[TimingPoints]') + 1\n",
    "        while raw_content[i] != '' and raw_content[i][0] != '[':\n",
    "            timing_points.append(raw_content[i])\n",
    "            i += 1\n",
    "\n",
    "        # Check if multiple BPMs exist\n",
    "        beat_lengths = {float(tp.split(',')[1]) for tp in timing_points if float(tp.split(',')[1]) > 0}\n",
    "        if len(beat_lengths) > 1:\n",
    "            return -1, -1, -1, -1, -1\n",
    "        \n",
    "        # Make sure beatmap contains only one meter of 4\n",
    "        meters = {int(tp.split(',')[2]) for tp in timing_points if float(tp.split(',')[1]) > 0}\n",
    "        if len(meters) > 1:\n",
    "            return -1, -1, -1, -1, -1\n",
    "        if list(meters)[0] != 4:\n",
    "            return -1, -1, -1, -1, -1\n",
    "\n",
    "        # Get offset and beat length\n",
    "        offset = float(timing_points[0].split(',')[0])\n",
    "        beat_length = beat_lengths.pop()\n",
    "\n",
    "        # Parse and convert beat objects\n",
    "        beatmap_start_index = raw_content.index('[HitObjects]')\n",
    "        beatmap = raw_content[beatmap_start_index + 1:]\n",
    "\n",
    "        obj_list = []\n",
    "        xpos_set = set()\n",
    "        for obj in beatmap:\n",
    "            obj_split = obj.split(',')\n",
    "            time = int(obj_split[2])\n",
    "            xpos = int(obj_split[0])\n",
    "            xpos_set.add(xpos)\n",
    "\n",
    "            if int(obj_split[3]) > 6: # If note is long note...\n",
    "                end_time = int(obj_split[5].split(':', 1)[0])\n",
    "                obj_list.append([time, xpos, 2])\n",
    "                obj_list.append([end_time, xpos, 3])\n",
    "            else:\n",
    "                obj_list.append([time, xpos, 1])\n",
    "\n",
    "        # Convert X-position to key number\n",
    "        xpos_list = sorted(xpos_set)\n",
    "        num_keys = len(xpos_list)\n",
    "        xpos2num = {xpos: num for num, xpos in enumerate(xpos_list)}\n",
    "        obj_list = [[obj[0], xpos2num[obj[1]], obj[2]] for obj in obj_list]\n",
    "\n",
    "        # Convert to tensor and sort by note time\n",
    "        obj_tensor = torch.tensor(obj_list, dtype=torch.float32)\n",
    "        obj_tensor = obj_tensor[obj_tensor[:, 0].argsort()]\n",
    "\n",
    "        return obj_tensor, num_keys, offset, beat_length, difficulty\n",
    "\n",
    "    def convert_audio(self, y, offset, beat_length, eps=1e-9):\n",
    "        \"\"\"\n",
    "        Converts audio into 3-channel mel-spectrogram with context windows.\n",
    "        INPUT: waveform of sr=44100, offset, beat length\n",
    "        RETURNS: Spectrogram: Tensor([num_timesteps, 80, 3]),\n",
    "                 Beat phase: Tensor([num_timesteps]),\n",
    "                 Beat num: Tensor([num_timesteps])\n",
    "        \"\"\"\n",
    "\n",
    "        # Multiple-timescale STFT\n",
    "        specs = []\n",
    "        for converter in self.melspec_converters:\n",
    "            melspec = converter(y + eps)\n",
    "            specs.append(torch.log(melspec.T))\n",
    "        specs = torch.stack(specs, dim=-1) # len X 80 X 3\n",
    "\n",
    "        # Create beat phase tensor\n",
    "        beat_phase = self.get_beat_phase(torch.arange(len(specs)) * self.hop_ms, offset, beat_length)\n",
    "        beat_num = self.get_beat_num(torch.arange(len(specs)) * self.hop_ms, beat_length, 4, offset)\n",
    "\n",
    "        return specs, beat_phase, beat_num\n",
    "    \n",
    "    def combination_to_index(self, combination, num_keys):\n",
    "        index = 0\n",
    "        for i, value in enumerate(combination):\n",
    "            index += value * (num_keys ** i)\n",
    "        return index\n",
    "\n",
    "    def index_to_combination(self, index, num_keys):\n",
    "        combination = []\n",
    "        for i in range(num_keys):\n",
    "            value = (index // (num_keys ** i)) % num_keys\n",
    "            combination.append(value)\n",
    "        return tuple(combination)\n",
    "    \n",
    "    def quantize_beatmap(self, beatmap, num_timesteps, num_keys):\n",
    "        \"\"\"\n",
    "        Quantizes beat objects to the grid of hop_ms.\n",
    "        INPUT: Beatmap: Tensor([num_notes X 3(time, key_number, note_type)])\n",
    "        RETURNS: Actions: Tensor([num_timesteps, 1])\n",
    "                 Onset: Tensor([num_timesteps, 1])\n",
    "        \"\"\"\n",
    "\n",
    "        # Quantize timings to hop_ms\n",
    "        beatmap_new = beatmap.clone()\n",
    "        timesteps = self.round_base(beatmap_new[:, 0], self.hop_ms) / self.hop_ms\n",
    "        beatmap_new[:, 0] = timesteps\n",
    "\n",
    "        # Create action tensor whose length matches with spectrogram\n",
    "        actions = torch.zeros([num_timesteps, num_keys])\n",
    "        for obj in beatmap_new:\n",
    "            timestep, key_number, note_type = obj.tolist()\n",
    "            actions[int(timestep), int(key_number)] = note_type\n",
    "\n",
    "        actions = torch.tensor([self.combination_to_index(obj.tolist(), num_keys) for obj in actions])\n",
    "        onsets = actions.bool().int()\n",
    "\n",
    "        return actions, onsets\n",
    "\n",
    "    def find_file_by_stem(self, fn_list, stem):\n",
    "        for fn in fn_list:\n",
    "            if fn.stem == stem:\n",
    "                return fn\n",
    "        return -1\n",
    "    \n",
    "    def move_file(self, old_path:Path, new_path:Path, overwrite=True):\n",
    "        if new_path.exists():\n",
    "            if overwrite:\n",
    "                new_path.unlink()\n",
    "                old_path.rename(new_path)\n",
    "            else:\n",
    "                old_path.unlink()\n",
    "        else:\n",
    "            old_path.rename(new_path)\n",
    "\n",
    "    def convert(self):\n",
    "        audio_suffixes = {'.mp3', '.MP3', '.wav', '.WAV', '.ogg', '.OGG'}\n",
    "        audio_fns = sorted([p for p in self.audio_path.glob('**/*') if p.suffix in audio_suffixes])\n",
    "        osu_fns = sorted(list(self.osu_path.glob('*.osu')))\n",
    "\n",
    "        # Create needed paths\n",
    "        excluded_osu_path = self.osu_path / 'excluded_osu'\n",
    "        excluded_osu_path.mkdir(exist_ok=True)\n",
    "        num_keys_paths = [] # Separate converted beatmaps by num_keys\n",
    "\n",
    "        log = open(self.beatmap_output_path / (strftime('conversion-log-%Y-%m-%d-%H-%M-%S', localtime()) + '.txt'), 'w')\n",
    "\n",
    "        for osu_fn in tqdm(osu_fns):\n",
    "            # Parse beatmap notes\n",
    "            beatmap, num_keys, offset, beat_length, difficulty = self.parse_beatmap(osu_fn)\n",
    "            if (num_keys == -1):\n",
    "                log.write(f'ERROR {osu_fn.name}: Incompatible beatmap. Skipping conversion.\\n')\n",
    "                self.move_file(osu_fn, excluded_osu_path / osu_fn.name)\n",
    "                continue\n",
    "\n",
    "            # Check if num_keys path already exists\n",
    "            num_keys_path = self.beatmap_output_path / f'{num_keys}keys/'\n",
    "            if num_keys_path not in num_keys_paths:\n",
    "                num_keys_path.mkdir(exist_ok=True)\n",
    "                num_keys_paths.append(num_keys_path)\n",
    "\n",
    "            # Check if corresponding audio has already been converted\n",
    "            audio_stem = osu_fn.stem.split('-')[0]\n",
    "            converted_audio_fn = self.find_file_by_stem(list(self.audio_output_path.glob('*.pt')), audio_stem)\n",
    "\n",
    "            num_timesteps = 0\n",
    "            if converted_audio_fn == -1: # If audio hasn't been converted...\n",
    "                # Load audio with OS-specific backend\n",
    "                audio_fn = self.find_file_by_stem(audio_fns, audio_stem)\n",
    "                if audio_fn == -1:\n",
    "                    log.write(f'ERROR {osu_fn.name}: Audio file not found. Skipping conversion.\\n')\n",
    "                    self.move_file(osu_fn, excluded_osu_path / osu_fn.name)\n",
    "                    continue\n",
    "\n",
    "                if is_macos:\n",
    "                    y, sr = torchaudio.load(audio_fn, backend='ffmpeg')\n",
    "                else:\n",
    "                    y, sr = torchaudio.load(audio_fn)\n",
    "\n",
    "                # Mono and resample\n",
    "                y = y.mean(dim=0)\n",
    "                if sr != 44100:\n",
    "                    log.write(f'WARN Sampling rate of file {audio_fn.name} is {sr}: Resampling to 44100.\\n')\n",
    "                    y = torchaudio.functional.resample(y, sr, 44100)\n",
    "\n",
    "                # Convert audio and save\n",
    "                specs, beat_phase, beat_num = self.convert_audio(y, offset, beat_length)\n",
    "                num_timesteps = len(specs)\n",
    "                torch.save({'specs': specs, 'beat_phase': beat_phase, 'beat_num': beat_num},\n",
    "                           (self.audio_output_path / f'{audio_fn.stem}.pt'))\n",
    "\n",
    "            else: # If audio has already been converted...\n",
    "                specs, beat_phase, beat_num = torch.load(converted_audio_fn).values()\n",
    "                num_timesteps = len(specs)\n",
    "\n",
    "            # Quantize beatmap and save\n",
    "            actions, onsets = self.quantize_beatmap(beatmap, num_timesteps, num_keys)\n",
    "            if not len(specs) == len(beat_phase) == len(beat_num) == len(actions) == len(onsets):\n",
    "                log.write(f'ERROR {osu_fn.name}: Features dimensions mismatch. Skipping conversion.\\n')\n",
    "                self.move_file(osu_fn, excluded_osu_path / osu_fn.name)\n",
    "                continue\n",
    "        \n",
    "            torch.save({'actions': actions, 'onsets': onsets, 'beatmap': beatmap, 'difficulty': difficulty},\n",
    "                       (num_keys_path / f'{osu_fn.stem}.pt'))\n",
    "\n",
    "        log.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = BeatmapConverter(Path('.'), Path('.'), Path('.'))\n",
    "test_fn = Path('osu_dataset/1003093-216-4.osu')\n",
    "test_audio = Path('osu_dataset/1003093.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[      241.,         1.,         1.],\n",
      "        [      241.,         2.,         1.],\n",
      "        [      469.,         3.,         1.],\n",
      "        ...,\n",
      "        [    87741.,         3.,         3.],\n",
      "        [    87741.,         1.,         1.],\n",
      "        [    87912.,         2.,         1.]])\n",
      "4\n",
      "1151.0\n",
      "454.545454545455\n",
      "2.2\n"
     ]
    }
   ],
   "source": [
    "beatmap, num_keys, offset, beat_length, difficulty = converter.parse_beatmap(test_fn)\n",
    "print(beatmap)\n",
    "print(num_keys)\n",
    "print(offset)\n",
    "print(beat_length)\n",
    "print(difficulty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9102, 80, 3])\n",
      "torch.Size([9102])\n",
      "torch.Size([9102])\n"
     ]
    }
   ],
   "source": [
    "y, sr = torchaudio.load(test_audio)\n",
    "y = y.mean(dim=0)\n",
    "if sr != 44100:\n",
    "    y = torchaudio.functional.resample(y, sr, 44100)\n",
    "\n",
    "specs, beat_phase, beat_num = converter.convert_audio(y, offset, beat_length)\n",
    "print(specs.shape)\n",
    "print(beat_phase.shape)\n",
    "print(beat_num.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "actions, onsets = converter.quantize_beatmap(beatmap, len(specs), num_keys)\n",
    "print(actions)\n",
    "print(onsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 0, 0, 3)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter.index_to_combination(194, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 8641, 1: 461})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(onsets.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion finished.08-7.osu (191 out of 192)\n"
     ]
    }
   ],
   "source": [
    "audio_path = Path('osu_dataset/original/')\n",
    "osu_path = Path('osu_dataset/original/')\n",
    "output_path = Path('osu_dataset/')\n",
    "\n",
    "converter = BeatmapConverter(audio_path, osu_path, output_path)\n",
    "converter.convert()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
